ğŸ¤– AI Cold Call Simulation Agent

An AI-powered B2B cold call simulator that behaves like a real human sales representative using Groq LLaMA 3.1-8B.

It qualifies prospects, detects business relevance, and produces a structured outcome â€” all within 40-60 seconds, just like real cold calls.

ğŸ¯ Overview

This project simulates real-world cold calls to:

âœ… Sound human (not scripted)
âœ… Qualify decision makers
âœ… Detect business relevance
âœ… End unsuitable calls quickly
âœ… Keep calls short and realistic
âœ… Produce structured qualification outcome

ğŸ—ï¸ Architecture Overview
Frontend (React + Vite)
        â”‚
        â”‚ REST API
        â–¼
FastAPI Backend (server.py)
        â”‚
        â–¼
LearningAgent (bot.py)
        â”‚
        â–¼
Groq LLM API (LLaMA 3.1-8B)

ğŸ”„ Call Flow

1ï¸âƒ£ User clicks Start Call

2ï¸âƒ£ Backend creates LearningAgent

3ï¸âƒ£ Agent asks opening question

4ï¸âƒ£ User replies via /next

5ï¸âƒ£ Agent:

Applies âš¡ Fast rule checks

Uses ğŸ§  LLM for reasoning

Maintains ğŸ§¾ short memory

6ï¸âƒ£ Agent either:

âœ” Continues conversation
âŒ Ends call
ğŸ“Š Returns qualification summary

ğŸ§© Component Breakdown
ğŸ¨ Frontend (React + Vite)

â€¢ Chat-style UI
â€¢ Calls /start and /next
â€¢ Displays replies and final results

âš™ï¸ FastAPI Backend

â€¢ Handles HTTP endpoints
â€¢ Maintains active agent instance
â€¢ Returns structured JSON

ğŸ§  LearningAgent (Core Brain)

Handles:

â€¢ Conversation state
â€¢ Regex-based early exits
â€¢ LLM reasoning
â€¢ Call timing logic
â€¢ Final structured summary

ğŸš€ Groq LLM

Model: llama-3.1-8b-instant

Used for:

â€¢ Natural human conversation
â€¢ Business reasoning
â€¢ Qualification decisions
â€¢ Polite call termination

ğŸ’¡ Key Design Decisions
âš¡ Hybrid Logic (Rules + LLM)

Why not pure LLM?

âŒ Expensive
âŒ Unpredictable

Solution:

âœ” Regex â†’ Fast exits
âœ” LLM â†’ Human reasoning

Benefits:

â€¢ Faster
â€¢ Cheaper
â€¢ Stable

ğŸ§¾ Short Conversation Memory

Only last 5-6 turns sent to LLM

Why:

â€¢ Lower cost
â€¢ Faster response
â€¢ Clean context

ğŸ“¦ Strict JSON Output

LLM forced to return:

{
"action": "continue | end",
"reply": "text"
}


Why:

â€¢ Prevent crashes
â€¢ Stable backend
â€¢ No hallucinated formats

ğŸ›¡ï¸ Safe Fallback Protection

If LLM fails:

âœ” System DOES NOT crash
âœ” Safe default reply provided

Production-ready safety.

â±ï¸ Call Duration Control

Cold calls must be short.

System exits if:

â€¢ Not owner
â€¢ Not interested
â€¢ Non-IT business
â€¢ Busy

ğŸ§  Stateful Agent (Stateless API)

API â†’ Stateless
Agent â†’ Stateful (in memory)

Simple & efficient for simulation.

âš–ï¸ Tradeoffs
ğŸ§  In-Memory Agent

âŒ Lost if server restarts

âœ” Simpler
âœ” Faster

Production â†’ Redis recommended

âš¡ Regex Early Signals

âŒ Not perfect

âœ” Very fast
âœ” Efficient

LLM handles complex reasoning.

ğŸ’° LLM Cost Optimization

LLM used only when needed.

Not every message â†’ LLM call.

âœ‚ Short Responses

Default: Max 3 sentences

Realistic cold-call behavior.

ğŸ”“ No Authentication

Open API for demo.

Production â†’ Add Auth Layer

ğŸ§  Qualification Logic Summary

Agent exits if:

âŒ Not owner
âŒ Not interested
âŒ Non-IT business

Agent continues if:

âœ” IT / Cloud business
âœ” Decision maker
âœ” Shows interest

Final output:

ğŸ“Š Structured qualification result

ğŸš€ How To Run
Backend Setup
py -3.11 -m venv .venv

.venv\Scripts\activate

pip install -r requirements.txt

python -m uvicorn server:app --reload

Frontend Setup
cd frontend

npm install

npm run dev

ğŸ§° Tech Stack
Backend

ğŸ Python 3.11
âš¡ FastAPI
ğŸš€ Groq API
ğŸ§  LLaMA 3.1

Frontend

âš› React
âš¡ Vite
ğŸŒ Axios

LLM

ğŸ§  llama-3.1-8b-instant
âš¡ Ultra-fast inference

ğŸ”® Future Improvements

ğŸš€ Redis session storage

ğŸ“Š Call analytics dashboard

ğŸ“ Voice integration (Twilio)

ğŸ“… Callback scheduling

ğŸ“ˆ Qualification scoring

ğŸ’° Cost tracking

ğŸ‘¥ Multi-call simulation

ğŸ¥ Demo Purpose

This project demonstrates:

âœ” AI agent design
âœ” Production-safe LLM usage
âœ” Hybrid AI architecture
âœ” Real-world sales simulation

ğŸ‘¨â€ğŸ’» Author

Ashhar Raza

Full Stack Developer | AI | Backend | Cloud

â­ If you like this project

Give it a â­ on GitHub